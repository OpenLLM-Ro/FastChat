Official code used for evaluating Romanian LLMs as proposed in [Masala et al. 2024](https://arxiv.org/abs/2406.18266). This repo is a fork of the popular [FastChat](https://github.com/lm-sys/FastChat)) repo used for LLM-as-a-Judge evaluation. 

In this package, you can use (Ro)MTBench and RoCulturaBench questions and prompts to evaluate your models with LLM-as-a-judge.
(Ro)MTBench is a set of challenging multi-turn open-ended questions for evaluating chat assistants, while RoCulturaBench evaluates how grounded an LLM is in Romanian culture.
To automate the evaluation process, we prompt strong LLMs like GPT-4 to act as judges and assess the quality of the models' responses.


## Contents
- [Install](#install)
- [Review Pre-Generated Model Answers and Judgments](#review-pre-generated-model-answers-and-judgments)
- [Evaluate](#evaluate)
- [Citation](#citation)

## Install
```
git clone https://github.com/lm-sys/FastChat.git
cd FastChat
pip install -e ".[model_worker,llm_judge]"
```

## Review Pre-Generated Model Answers and Judgments
Model answers and judgments will be uploaded soon.
<!--
After downloading the data, you can view them locally by
```
python3 qa_browser.py --share
```
You can use this QA browser to view the answers generated by you later.
-->

## Evaluate 

### Step 1. Generate model answers to questions
```
python gen_model_answer.py --bench-name [BENCHMARK] --model-path [MODEL-PATH] --model-id [MODEL-ID]
```
Arguments:
  - `[BENCHMARK]` is the benchmark name: `mt_bench_ro` or `cultura_bench_ro`
  - `[MODEL-PATH]` is the path to the weights, which can be a local folder or a Hugging Face repo ID.
  - `[MODEL-ID]` is a name you give to the model.
  - Optionally, you can use `--force_ro` to specifically ask the model to answer each question in Romanian.

e.g.,
```
python gen_model_answer.py --bench-name=mt_bench_ro --model-path OpenLLM-Ro/RoLlama2-7b-Instruct --model-id RoLlama2-7b-Instruct
```

The answers will be saved to `data/mt_bench/model_answer/[MODEL-ID].jsonl`.

To make sure FastChat loads the correct prompt template, see the supported models and how to add a new model [here](../../docs/model_support.md#how-to-support-a-new-model).

You can also specify `--num-gpus-per-model` for model parallelism (needed for large 65B models) and `--num-gpus-total` to parallelize answer generation with multiple GPUs.

### Step 2. Generate GPT-4 judgments
There are several options to use GPT-4 as a judge, such as pairwise winrate and single-answer grading.
We recommend single-answer grading as the default mode.
This mode asks GPT-4 to grade and give a score to model's answer directly without pairwise comparison.
For each turn, GPT-4 will give a score on a scale of 10. We then compute the average score on all turns.

```
export OPENAI_API_KEY=XXXXXX  # set the OpenAI API key
python gen_judgment.py --bench-name [BENCHMARK] --judge-file="data/judge_prompts_ro.jsonl" --batch_requests --model-list [LIST-OF-MODEL-ID] --parallel [num-concurrent-api-call]

```

e.g.,
```
python gen_judgment.py --bench-name="cultura_bench_ro" --judge-file="data/judge_prompts_ro.jsonl" --batch_requests --model-list RoLlama2-7b-Instruct
```
We stronly recommend to use the `--batch_requests` flags as it uses OpenAI Batch API and is cheaper. The judgments will be saved to `data/[BENCHMARK]/model_judgment/[JUDGE-MODEL].jsonl`. We recommend the newest GPT-4o model as judge: gpt-4o-2024-08-06. 

### Step 3. Show MT-bench scores

- Show the scores for selected models
  ```
  python show_result.py --model-list vicuna-13b-v1.3 alpaca-13b llama-13b claude-v1 gpt-3.5-turbo gpt-4
  ```
- Show all scores
  ```
  python show_result.py
  ```

---

### How to plot the radar figure?

You can use this [colab notebook](https://colab.research.google.com/drive/15O3Y8Rxq37PuMlArE291P4OC6ia37PQK#scrollTo=5i8R0l-XqkgO) to plot the radar figure for MT-bench.

<img src="data/mt_bench/misc/radar.png" width="600" height="450">



## Citation

```bibtex
@misc{masala2024vorbecstiromanecsterecipetrain,
      title={"Vorbe\c{s}ti Rom\^ane\c{s}te?" A Recipe to Train Powerful Romanian LLMs with English Instructions}, 
      author={Mihai Masala and Denis C. Ilie-Ablachim and Alexandru Dima and Dragos Corlatescu and Miruna Zavelca and Ovio Olaru and Simina Terian and Andrei Terian and Marius Leordeanu and Horia Velicu and Marius Popescu and Mihai Dascalu and Traian Rebedea},
      year={2024},
      eprint={2406.18266},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18266}, 
}
```

### Acknowledgement
This repo benefits from [FastChat](https://github.com/lm-sys/FastChat). We thank them for their wonderful work.
